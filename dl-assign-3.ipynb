{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11873693,"sourceType":"datasetVersion","datasetId":7462032}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, GRU, RNN, Dense, Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:19.629195Z","iopub.execute_input":"2025-05-20T14:01:19.629349Z","iopub.status.idle":"2025-05-20T14:01:19.634198Z","shell.execute_reply.started":"2025-05-20T14:01:19.629334Z","shell.execute_reply":"2025-05-20T14:01:19.632908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import wandb\nfrom wandb.integration.keras import WandbCallback\n\nwandb.login(key='e030007b097df00d9a751748294abc8440f932b1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:19.634986Z","iopub.execute_input":"2025-05-20T14:01:19.635236Z","iopub.status.idle":"2025-05-20T14:01:28.456047Z","shell.execute_reply.started":"2025-05-20T14:01:19.635213Z","shell.execute_reply":"2025-05-20T14:01:28.455500Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmm21b044\u001b[0m (\u001b[33mmm21b044-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Question 1","metadata":{}},{"cell_type":"code","source":"def load_data(file_path):\n    df = pd.read_csv(file_path, sep='\\t', header=None, names=['latin', 'native'])\n    df = df.dropna()\n    df['latin'] = df['latin'].astype(str)\n    df['native'] = df['native'].astype(str)\n    return df\n\ndef load_dakshina_dataset(language_code='hi', base_dir='/kaggle/input/dak-data/dakshina_dataset_v1.0'):\n    path = os.path.join(base_dir, language_code, 'lexicons')\n    return (\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.train.tsv')),\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.dev.tsv')),\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.test.tsv')),\n    )\n\ntrain_data, val_data, test_data = load_dakshina_dataset()\n\n# ─── 1) Extract raw texts ─────────────────────────────────────────\ninput_texts  = train_data['latin'].tolist()\ntarget_texts = ['\\t' + t + '\\n' for t in train_data['native'].tolist()]\n\nval_input_texts  = val_data['latin'].tolist()\nval_target_texts = ['\\t' + t + '\\n' for t in val_data['native'].tolist()]\n\ninput_tokenizer = Tokenizer(char_level=True, oov_token=None)\ninput_tokenizer.fit_on_texts(input_texts + val_input_texts)\n\ntarget_tokenizer = Tokenizer(char_level=True, oov_token=None)\ntarget_tokenizer.fit_on_texts(target_texts + val_target_texts)\n\n# ─── 3) Convert texts → integer sequences + pad to max lengths ───\n# Compute max lengths\nmax_in  = max(len(txt) for txt in input_texts + val_input_texts)\nmax_out = max(len(txt) for txt in target_texts + val_target_texts)\n\n# Integer‑encode + pad\nencoder_input_train = pad_sequences(\n    input_tokenizer.texts_to_sequences(input_texts),\n    maxlen=max_in,\n    padding='post'\n)\ndecoder_input_train = pad_sequences(\n    target_tokenizer.texts_to_sequences(target_texts),\n    maxlen=max_out,\n    padding='post'\n)\n# decoder targets are the decoder inputs shifted left by one\ndecoder_target_train = np.array(decoder_input_train)[:, 1:]\ndecoder_input_train   = np.array(decoder_input_train)[:, :-1]\n\n# Do the same for validation set\nencoder_input_val = pad_sequences(\n    input_tokenizer.texts_to_sequences(val_input_texts),\n    maxlen=max_in,\n    padding='post'\n)\ndecoder_input_val = pad_sequences(\n    target_tokenizer.texts_to_sequences(val_target_texts),\n    maxlen=max_out,\n    padding='post'\n)\ndecoder_target_val = np.array(decoder_input_val)[:, 1:]\ndecoder_input_val   = np.array(decoder_input_val)[:, :-1]\n\n# Make sure any previous wandb runs are finished\ntry:\n    wandb.finish()\nexcept:\n    pass\n\n# Initialize wandb with proper error handling\ntry:\n    wandb.init(\n        project=\"DA_seq2seq_transliteration\",\n        name=\"vanilla_lstm_run_q1\",\n        # Removed reinit=True to prevent connection issues\n        config={\n            \"model_type\": \"vanilla\",\n            \"cell_type\": \"LSTM\",\n            \"embedding_dim\": 64,\n            \"hidden_dim\": 128,\n            \"dropout_rate\": 0.2,\n            \"batch_size\": 64,\n            \"epochs\": 10,\n            \"input_vocab_size\": len(input_tokenizer.word_index) + 1,\n            \"target_vocab_size\": len(target_tokenizer.word_index) + 1,\n            \"max_input_length\": max_in,\n            \"max_target_length\": max_out,\n            \"optimizer\": \"adam\",\n            \"loss\": \"sparse_categorical_crossentropy\",\n            \"dataset\": \"dakshina_hi\"\n        }\n    )\nexcept Exception as e:\n    print(f\"Failed to initialize wandb: {e}\")\n    # Create a dummy wandb to avoid errors in the code\n    class DummyWandb:\n        def log(self, *args, **kwargs):\n            pass\n        def config(self, *args, **kwargs):\n            return type('obj', (object,), {\n                'embedding_dim': 64,\n                'hidden_dim': 128,\n                'get': lambda s, k, d: d\n            })\n    wandb = DummyWandb()\n\nclass VanillaSeq2Seq:\n    def __init__(self,\n                 input_vocab_size,\n                 target_vocab_size,\n                 embedding_dim,\n                 hidden_dim,\n                 cell_type='LSTM',\n                 dropout_rate=0.2,\n                 num_encoder_layers=1,\n                 num_decoder_layers=1):\n        self.input_vocab_size  = input_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.embedding_dim     = embedding_dim\n        self.hidden_dim        = hidden_dim\n        self.cell_type         = cell_type\n        self.dropout_rate      = dropout_rate\n        self.num_encoder_layers = num_encoder_layers\n        self.num_decoder_layers = num_decoder_layers\n        self.model = self._build_model()\n\n    def _rnn_layer(self, return_sequences, return_state):\n        \"\"\"Factory for one RNN/LSTM/GRU layer.\"\"\"\n        if self.cell_type == 'LSTM':\n            return LSTM(self.hidden_dim,\n                        return_sequences=return_sequences,\n                        return_state=return_state)\n        elif self.cell_type == 'GRU':\n            return GRU(self.hidden_dim,\n                       return_sequences=return_sequences,\n                       return_state=return_state)\n        else:\n            return RNN(self.hidden_dim,\n                       return_sequences=return_sequences,\n                       return_state=return_state)\n\n    def _build_model(self):\n        encoder_inputs = Input(shape=(None,), name='encoder_input')\n        x = Embedding(self.input_vocab_size, self.embedding_dim)(encoder_inputs)\n        x = Dropout(self.dropout_rate)(x)\n\n        # Stack encoder layers\n        encoder_states = []\n        for i in range(self.num_encoder_layers):\n            # last encoder layer returns only state, earlier ones return sequences\n            rs = (i < self.num_encoder_layers - 1)\n            if self.cell_type == 'LSTM':\n                x, state_h, state_c = LSTM(\n                    self.hidden_dim,\n                    return_sequences=rs,\n                    return_state=True,\n                    name=f'enc_lstm_{i}'\n                )(x)\n                encoder_states = [state_h, state_c]\n            else:\n                x, state_h = self._rnn_layer(\n                    return_sequences=rs,\n                    return_state=True\n                )(x)\n                encoder_states = [state_h]\n\n        decoder_inputs = Input(shape=(None,), name='decoder_input')\n        y = Embedding(self.target_vocab_size, self.embedding_dim)(decoder_inputs)\n        y = Dropout(self.dropout_rate)(y)\n\n        # Stack decoder layers\n        for i in range(self.num_decoder_layers):\n            rs = True  # decoder always returns sequences for all but we only care about final dense\n            if self.cell_type == 'LSTM':\n                # feed initial_state only to the first decoder layer\n                init_st = encoder_states if i == 0 else None\n                y, dh, dc = LSTM(\n                    self.hidden_dim,\n                    return_sequences=True,\n                    return_state=True,\n                    name=f'dec_lstm_{i}'\n                )(y, initial_state=init_st) if init_st else LSTM(\n                    self.hidden_dim,\n                    return_sequences=True,\n                    return_state=True,\n                    name=f'dec_lstm_{i}'\n                )(y)\n            else:\n                init_st = encoder_states if i == 0 else None\n                y, dh = self._rnn_layer(\n                    return_sequences=True,\n                    return_state=True\n                )(y, initial_state=init_st) if init_st else self._rnn_layer(\n                    return_sequences=True,\n                    return_state=True\n                )(y)\n\n        # Final projection\n        outputs = Dense(self.target_vocab_size, activation='softmax')(y)\n        return Model([encoder_inputs, decoder_inputs], outputs)\n\n    def compile(self, optimizer='adam', loss='sparse_categorical_crossentropy'):\n        self.model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    def fit(self, train_data, val_data, batch_size=64, epochs=8, callbacks=None):\n        return self.model.fit(\n            [train_data['encoder_input'], train_data['decoder_input']],\n            np.expand_dims(train_data['decoder_target'], -1),\n            validation_data=(\n                [val_data['encoder_input'], val_data['decoder_input']],\n                np.expand_dims(val_data['decoder_target'], -1)\n            ),\n            batch_size=batch_size,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n        \ninput_vocab_size = len(input_tokenizer.word_index) + 1\ntarget_vocab_size = len(target_tokenizer.word_index) + 1\n\nmodel = VanillaSeq2Seq(\n    input_vocab_size=input_vocab_size,\n    target_vocab_size=target_vocab_size,\n    embedding_dim=64,\n    hidden_dim=128,\n    cell_type='LSTM',\n    dropout_rate=0.2\n)\n\nmodel.compile()\n\n# Modified wandb callback with error handling\ntry:\n    wandb_callback = WandbCallback(\n        log_model=False,           # no wandb artifact\n        save_graph=False,          # don't try to render graph\n        save_model=False           # ✅ disables all auto saving\n    )\n    callbacks = [wandb_callback]\nexcept Exception as e:\n    print(f\"Failed to initialize WandbCallback: {e}\")\n    callbacks = []\n\n# Use try-except for wandb config access\ntry:\n    D = wandb.config.embedding_dim\n    H = wandb.config.hidden_dim\n    L_e = wandb.config.get(\"num_encoder_layers\", 1)\n    L_d = wandb.config.get(\"num_decoder_layers\", 1)\nexcept Exception as e:\n    print(f\"Failed to access wandb config: {e}\")\n    D = 64  # Default values\n    H = 128\n    L_e = 1\n    L_d = 1\n\nT_enc = encoder_input_train.shape[1]\nT_dec = decoder_input_train.shape[1]\n\nflops_per_step = 4 * (H * D + H * H)\n\n# 4) Total ops over all layers & timesteps\ntotal_enc_flops = L_e * T_enc * flops_per_step\ntotal_dec_flops = L_d * T_dec * flops_per_step\ntotal_flops = total_enc_flops + total_dec_flops\n\nprint(f\"Approximate total multiplications (encoder + decoder): {total_flops:,}\")\n\ntotal_params = model.model.count_params()\nprint(f\"Total trainable parameters: {total_params:,}\")\n\nmodel.model.summary()\n\nhistory = model.fit(\n    train_data={\n        'encoder_input': encoder_input_train,\n        'decoder_input': decoder_input_train,\n        'decoder_target': decoder_target_train\n    },\n    val_data={\n        'encoder_input': encoder_input_val,\n        'decoder_input': decoder_input_val,\n        'decoder_target': decoder_target_val\n    },\n    batch_size=64,\n    epochs=8,\n    callbacks=callbacks\n)\n\n# Log metrics to wandb with error handling\ntry:\n    wandb.log({\n        \"total_flops\": total_flops,\n        \"total_trainable_params\": total_params\n    })\n    # Properly close the wandb run\n    wandb.finish()\nexcept Exception as e:\n    print(f\"Failed to log to wandb: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:01:58.565630Z","iopub.execute_input":"2025-05-20T14:01:58.565913Z","iopub.status.idle":"2025-05-20T14:02:54.916092Z","shell.execute_reply.started":"2025-05-20T14:01:58.565892Z","shell.execute_reply":"2025-05-20T14:02:54.915527Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_140159-r1yo9564</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/r1yo9564' target=\"_blank\">vanilla_lstm_run_q1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/r1yo9564' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/r1yo9564</a>"},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1747749727.720180      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1747749727.720823      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is deprecated and will be removed in a future release. Please use the WandbMetricsLogger, WandbModelCheckpoint, and WandbEvalCallback callbacks instead. See https://docs.wandb.ai/guides/integrations/keras for more information.\n","output_type":"stream"},{"name":"stdout","text":"Approximate total multiplications (encoder + decoder): 2,260,992\nTotal trainable parameters: 201,869\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │          \u001b[38;5;34m1,728\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │            \u001b[38;5;34m832\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ enc_lstm_0 (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │         \u001b[38;5;34m98,816\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n│                           │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dec_lstm_0 (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),    │         \u001b[38;5;34m98,816\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │ enc_lstm_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],      │\n│                           │ \u001b[38;5;34m128\u001b[0m)]                  │                │ enc_lstm_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)       │          \u001b[38;5;34m1,677\u001b[0m │ dec_lstm_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ encoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,728</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ enc_lstm_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dec_lstm_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │ enc_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],      │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]                  │                │ enc_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]       │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,677</span> │ dec_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201,869\u001b[0m (788.55 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,869</span> (788.55 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m201,869\u001b[0m (788.55 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,869</span> (788.55 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/8\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747749734.614468     131 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7907 - loss: 0.6139 - val_accuracy: 0.8282 - val_loss: 0.3884\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8258 - loss: 0.4008 - val_accuracy: 0.8293 - val_loss: 0.3774\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8283 - loss: 0.3912 - val_accuracy: 0.8322 - val_loss: 0.3763\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8310 - loss: 0.3900 - val_accuracy: 0.8346 - val_loss: 0.3753\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8341 - loss: 0.3864 - val_accuracy: 0.8374 - val_loss: 0.3713\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8370 - loss: 0.3810 - val_accuracy: 0.8403 - val_loss: 0.3711\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8407 - loss: 0.3774 - val_accuracy: 0.8440 - val_loss: 0.3640\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8410 - loss: 0.3755 - val_accuracy: 0.8440 - val_loss: 0.3626\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▅▆▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>loss</td><td>█▃▂▂▂▂▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_trainable_params</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▄▅▆██</td></tr><tr><td>val_loss</td><td>█▅▅▄▃▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.84121</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.36256</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>loss</td><td>0.37412</td></tr><tr><td>total_flops</td><td>2260992</td></tr><tr><td>total_trainable_params</td><td>201869</td></tr><tr><td>val_accuracy</td><td>0.84397</td></tr><tr><td>val_loss</td><td>0.36256</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vanilla_lstm_run_q1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/r1yo9564' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/r1yo9564</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_140159-r1yo9564/logs</code>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, GRU, SimpleRNN, Dense, Dropout\nfrom tensorflow.keras.models import Model\nimport os\nimport time\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:02:54.917424Z","iopub.execute_input":"2025-05-20T14:02:54.917711Z","iopub.status.idle":"2025-05-20T14:02:54.922176Z","shell.execute_reply.started":"2025-05-20T14:02:54.917694Z","shell.execute_reply":"2025-05-20T14:02:54.921526Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the Dakshina dataset (Hindi as an example)\n# You can change 'hi' to the language of your choice\ndef load_dakshina_data(lang='hi'):\n    base_path = f'/kaggle/input/dak-data/dakshina_dataset_v1.0/{lang}/lexicons/'\n    \n    # Load train, dev, test sets\n    train_data = pd.read_csv(f'{base_path}{lang}.translit.sampled.train.tsv', sep='\\t', \n                             header=None, names=['latin', 'native', 'class'])\n    val_data = pd.read_csv(f'{base_path}{lang}.translit.sampled.dev.tsv', sep='\\t', \n                           header=None, names=['latin', 'native', 'class'])\n    test_data = pd.read_csv(f'{base_path}{lang}.translit.sampled.test.tsv', sep='\\t', \n                            header=None, names=['latin', 'native', 'class'])\n\n    # Drop any rows with missing values\n    train_data = train_data.dropna().astype(str)\n    val_data   = val_data.dropna().astype(str)\n    test_data  = test_data.dropna().astype(str)\n\n    return train_data, val_data, test_data\n\n# Process data and create sequences\ndef process_data(train_data, val_data):\n    # Extract texts\n    input_texts = train_data['latin'].tolist()\n    target_texts = ['\\t' + t + '\\n' for t in train_data['native'].tolist()]\n    \n    val_input_texts = val_data['latin'].tolist()\n    val_target_texts = ['\\t' + t + '\\n' for t in val_data['native'].tolist()]\n    \n    # Build character-level tokenizers\n    input_tokenizer = Tokenizer(char_level=True, oov_token=None)\n    input_tokenizer.fit_on_texts(input_texts + val_input_texts)\n    \n    target_tokenizer = Tokenizer(char_level=True, oov_token=None)\n    target_tokenizer.fit_on_texts(target_texts + val_target_texts)\n    \n    # Find max lengths\n    max_in = max(len(txt) for txt in input_texts + val_input_texts)\n    max_out = max(len(txt) for txt in target_texts + val_target_texts)\n    \n    # Convert to sequences and pad\n    encoder_input_train = pad_sequences(\n        input_tokenizer.texts_to_sequences(input_texts),\n        maxlen=max_in,\n        padding='post'\n    )\n    decoder_input_train = pad_sequences(\n        target_tokenizer.texts_to_sequences(target_texts),\n        maxlen=max_out,\n        padding='post'\n    )\n    decoder_target_train = np.array(decoder_input_train)[:, 1:]\n    decoder_input_train = np.array(decoder_input_train)[:, :-1]\n    \n    # Same for validation set\n    encoder_input_val = pad_sequences(\n        input_tokenizer.texts_to_sequences(val_input_texts),\n        maxlen=max_in,\n        padding='post'\n    )\n    decoder_input_val = pad_sequences(\n        target_tokenizer.texts_to_sequences(val_target_texts),\n        maxlen=max_out,\n        padding='post'\n    )\n    decoder_target_val = np.array(decoder_input_val)[:, 1:]\n    decoder_input_val = np.array(decoder_input_val)[:, :-1]\n    \n    return {\n        'input_tokenizer': input_tokenizer,\n        'target_tokenizer': target_tokenizer,\n        'max_in': max_in,\n        'max_out': max_out,\n        'encoder_input_train': encoder_input_train,\n        'decoder_input_train': decoder_input_train,\n        'decoder_target_train': decoder_target_train,\n        'encoder_input_val': encoder_input_val,\n        'decoder_input_val': decoder_input_val,\n        'decoder_target_val': decoder_target_val,\n        'input_texts': input_texts,\n        'target_texts': target_texts,\n        'val_input_texts': val_input_texts,\n        'val_target_texts': val_target_texts\n    }\n\n# Seq2Seq model with configurable parameters\nclass VanillaSeq2Seq:\n    def __init__(self,\n                 input_vocab_size,\n                 target_vocab_size,\n                 embedding_dim,\n                 hidden_dim,\n                 cell_type='LSTM',\n                 dropout_rate=0.2,\n                 num_encoder_layers=1,\n                 num_decoder_layers=1):\n        self.input_vocab_size = input_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.cell_type = cell_type\n        self.dropout_rate = dropout_rate\n        self.num_encoder_layers = num_encoder_layers\n        self.num_decoder_layers = num_decoder_layers\n        self.model = self._build_model()\n        \n    def _rnn_layer(self, return_sequences, return_state):\n        \"\"\"Factory for one RNN/LSTM/GRU layer.\"\"\"\n        if self.cell_type == 'LSTM':\n            return LSTM(self.hidden_dim,\n                        return_sequences=return_sequences,\n                        return_state=return_state)\n        elif self.cell_type == 'GRU':\n            return GRU(self.hidden_dim,\n                       return_sequences=return_sequences,\n                       return_state=return_state)\n        else:  # 'RNN'\n            return SimpleRNN(self.hidden_dim,\n                             return_sequences=return_sequences,\n                             return_state=return_state)\n\n    \n    def _build_model(self):\n        encoder_inputs = Input(shape=(None,), name='encoder_input')\n        x = Embedding(self.input_vocab_size, self.embedding_dim)(encoder_inputs)\n        x = Dropout(self.dropout_rate)(x)\n        \n        # Stack encoder layers\n        encoder_states = []\n        for i in range(self.num_encoder_layers):\n            # last encoder layer returns only state, earlier ones return sequences\n            rs = (i < self.num_encoder_layers - 1)\n            if self.cell_type == 'LSTM':\n                x, state_h, state_c = LSTM(\n                    self.hidden_dim,\n                    return_sequences=rs,\n                    return_state=True,\n                    name=f'enc_lstm_{i}'\n                )(x)\n                encoder_states = [state_h, state_c]\n            else:\n                x, state_h = self._rnn_layer(\n                    return_sequences=rs,\n                    return_state=True\n                )(x)\n                encoder_states = [state_h]\n        \n        decoder_inputs = Input(shape=(None,), name='decoder_input')\n        y = Embedding(self.target_vocab_size, self.embedding_dim)(decoder_inputs)\n        y = Dropout(self.dropout_rate)(y)\n        \n        # Stack decoder layers\n        for i in range(self.num_decoder_layers):\n            rs = True  # decoder always returns sequences\n            if self.cell_type == 'LSTM':\n                # feed initial_state only to the first decoder layer\n                init_st = encoder_states if i == 0 else None\n                y, dh, dc = LSTM(\n                    self.hidden_dim, \n                    return_sequences=True,\n                    return_state=True,\n                    name=f'dec_lstm_{i}'\n                )(y, initial_state=init_st) if init_st else LSTM(\n                    self.hidden_dim,\n                    return_sequences=True,\n                    return_state=True,\n                    name=f'dec_lstm_{i}'\n                )(y)\n            else:\n                init_st = encoder_states if i == 0 else None\n                y, dh = self._rnn_layer(\n                    return_sequences=True,\n                    return_state=True\n                )(y, initial_state=init_st) if init_st else self._rnn_layer(\n                    return_sequences=True,\n                    return_state=True\n                )(y)\n        \n        # Final projection\n        outputs = Dense(self.target_vocab_size, activation='softmax')(y)\n        return Model([encoder_inputs, decoder_inputs], outputs)\n    \n    def compile(self, optimizer='adam', loss='sparse_categorical_crossentropy'):\n        self.model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n        \n    def fit(self, train_data, val_data, batch_size=64, epochs=8, callbacks=None):\n        return self.model.fit(\n            [train_data['encoder_input'], train_data['decoder_input']],\n            np.expand_dims(train_data['decoder_target'], -1),\n            validation_data=(\n                [val_data['encoder_input'], val_data['decoder_input']],\n                np.expand_dims(val_data['decoder_target'], -1)\n            ),\n            batch_size=batch_size,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n\ndef run_wandb_sweep(processed_data):\n    # Define sweep configuration\n    sweep_config = {\n        'method': 'bayes',\n        'metric': {\n            'name': 'val_accuracy',\n            'goal': 'maximize'\n        },\n        'parameters': {\n            'embedding_dim': {\n                'values': [16, 32, 64, 128]\n            },\n            'hidden_dim': {\n                'values': [32, 64, 128, 256]\n            },\n            'cell_type': {\n                'values': ['RNN', 'GRU', 'LSTM']\n            },\n            'dropout_rate': {\n                'values': [0.1, 0.2, 0.3]\n            },\n            'num_encoder_layers': {\n                'values': [1, 2]\n            },\n            'num_decoder_layers': {\n                'values': [1, 2]\n            }\n        }\n    }\n    \n    # Initialize sweep\n    sweep_id = wandb.sweep(sweep_config, project=\"DA_seq2seq_transliteration\")\n    \n    # Define the training function\n    def train_model():\n        # Make sure we're in a clean wandb state\n        try:\n            wandb.finish()\n        except:\n            pass\n        \n        # Start a new wandb run\n        run = wandb.init()\n        \n        # Access hyperparameters from wandb\n        config = wandb.config\n        \n        # Create model with hyperparameters from wandb\n        input_vocab_size = len(processed_data['input_tokenizer'].word_index) + 1\n        target_vocab_size = len(processed_data['target_tokenizer'].word_index) + 1\n        \n        model = VanillaSeq2Seq(\n            input_vocab_size=input_vocab_size,\n            target_vocab_size=target_vocab_size,\n            embedding_dim=config.embedding_dim,\n            hidden_dim=config.hidden_dim,\n            cell_type=config.cell_type,\n            dropout_rate=config.dropout_rate,\n            num_encoder_layers=config.num_encoder_layers,\n            num_decoder_layers=config.num_decoder_layers\n        )\n        \n        model.compile()\n        \n        # Configure wandb callback\n        wandb_callback = WandbCallback(\n            log_model=False,\n            save_graph=False,\n            save_model=False\n        )\n        \n        # Compute model complexity metrics\n        D = config.embedding_dim\n        H = config.hidden_dim\n        L_e = config.num_encoder_layers\n        L_d = config.num_decoder_layers\n        T_enc = processed_data['encoder_input_train'].shape[1]\n        T_dec = processed_data['decoder_input_train'].shape[1]\n        \n        flops_per_step = 4 * (H * D + H * H)\n        total_enc_flops = L_e * T_enc * flops_per_step\n        total_dec_flops = L_d * T_dec * flops_per_step\n        total_flops = total_enc_flops + total_dec_flops\n        \n        total_params = model.model.count_params()\n        \n        print(f\"Embedding dim: {D}, Hidden dim: {H}\")\n        print(f\"Encoder layers: {L_e}, Decoder layers: {L_d}\")\n        print(f\"Cell type: {config.cell_type}, Dropout: {config.dropout_rate}\")\n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Total FLOPs: {total_flops:,}\")\n        \n        # Log model complexity metrics\n        wandb.log({\n            \"total_flops\": total_flops,\n            \"total_params\": total_params\n        })\n        \n        # Train the model\n        start_time = time.time()\n        \n        history = model.fit(\n            train_data={\n                'encoder_input': processed_data['encoder_input_train'],\n                'decoder_input': processed_data['decoder_input_train'],\n                'decoder_target': processed_data['decoder_target_train']\n            },\n            val_data={\n                'encoder_input': processed_data['encoder_input_val'],\n                'decoder_input': processed_data['decoder_input_val'],\n                'decoder_target': processed_data['decoder_target_val']\n            },\n            batch_size=64,\n            epochs=8,\n            callbacks=[wandb_callback]\n        )\n        \n        training_time = time.time() - start_time\n        \n        # Log additional metrics\n        wandb.log({\n            \"training_time\": training_time,\n            \"final_train_accuracy\": history.history['accuracy'][-1],\n            \"final_val_accuracy\": history.history['val_accuracy'][-1]\n        })\n        \n        # Clean up (important to avoid memory leaks)\n        tf.keras.backend.clear_session()\n        run.finish()\n    \n    # Run the sweep\n    wandb.agent(sweep_id, train_model, count=7)  # Adjust count based on time constraints\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:02:54.922985Z","iopub.execute_input":"2025-05-20T14:02:54.923169Z","iopub.status.idle":"2025-05-20T14:02:55.248884Z","shell.execute_reply.started":"2025-05-20T14:02:54.923155Z","shell.execute_reply":"2025-05-20T14:02:55.247994Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    # Ensure TensorFlow doesn't reserve all GPU memory\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    \n    # Load data\n    try:\n        train_data, val_data, test_data = load_dakshina_data(lang='hi')\n        print(f\"Data loaded successfully! Train size: {len(train_data)}\")\n        \n        # Process data\n        processed_data = process_data(train_data, val_data)\n        print(\"Data processed successfully!\")\n        \n        # Run sweep\n        print(\"Starting hyperparameter sweep...\")\n        run_wandb_sweep(processed_data)\n        \n    except Exception as e:\n        print(f\"Error: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T08:02:06.116449Z","iopub.execute_input":"2025-05-20T08:02:06.116777Z","iopub.status.idle":"2025-05-20T10:01:42.693865Z","shell.execute_reply.started":"2025-05-20T08:02:06.116755Z","shell.execute_reply":"2025-05-20T10:01:42.693033Z"}},"outputs":[{"name":"stdout","text":"Data loaded successfully! Train size: 44202\nData processed successfully!\nStarting hyperparameter sweep...\nCreate sweep with ID: mpq5soqm\nSweep URL: https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: inksfj02 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_080214-inksfj02</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/inksfj02' target=\"_blank\">charmed-sweep-1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/inksfj02' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/inksfj02</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is deprecated and will be removed in a future release. Please use the WandbMetricsLogger, WandbModelCheckpoint, and WandbEvalCallback callbacks instead. See https://docs.wandb.ai/guides/integrations/keras for more information.\n","output_type":"stream"},{"name":"stdout","text":"Embedding dim: 64, Hidden dim: 256\nEncoder layers: 2, Decoder layers: 1\nCell type: LSTM, Dropout: 0.3\nTotal parameters: 1,196,125\nTotal FLOPs: 19,333,120\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 196ms/step - accuracy: 0.7037 - loss: 1.1084 - val_accuracy: 0.7615 - val_loss: 0.7974\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 194ms/step - accuracy: 0.7614 - loss: 0.7937 - val_accuracy: 0.8196 - val_loss: 0.5974\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 196ms/step - accuracy: 0.8165 - loss: 0.5964 - val_accuracy: 0.8607 - val_loss: 0.4616\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 198ms/step - accuracy: 0.8617 - loss: 0.4554 - val_accuracy: 0.9029 - val_loss: 0.3273\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 196ms/step - accuracy: 0.8995 - loss: 0.3332 - val_accuracy: 0.9259 - val_loss: 0.2486\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 191ms/step - accuracy: 0.9226 - loss: 0.2538 - val_accuracy: 0.9383 - val_loss: 0.2057\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 193ms/step - accuracy: 0.9357 - loss: 0.2088 - val_accuracy: 0.9444 - val_loss: 0.1811\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 193ms/step - accuracy: 0.9436 - loss: 0.1793 - val_accuracy: 0.9482 - val_loss: 0.1700\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▂▄▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▅▃▂▂▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇███</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.9442</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.17003</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.9442</td></tr><tr><td>final_val_accuracy</td><td>0.9482</td></tr><tr><td>loss</td><td>0.17732</td></tr><tr><td>total_flops</td><td>19333120</td></tr><tr><td>total_params</td><td>1196125</td></tr><tr><td>training_time</td><td>1083.24335</td></tr><tr><td>val_accuracy</td><td>0.9482</td></tr><tr><td>val_loss</td><td>0.17003</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">charmed-sweep-1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/inksfj02' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/inksfj02</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_080214-inksfj02/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0r31b8ft with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_082035-0r31b8ft</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/0r31b8ft' target=\"_blank\">robust-sweep-2</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/0r31b8ft' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/0r31b8ft</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 16, Hidden dim: 32\nEncoder layers: 1, Decoder layers: 2\nCell type: GRU, Dropout: 0.2\nTotal parameters: 18,381\nTotal FLOPs: 374,784\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 33ms/step - accuracy: 0.6691 - loss: 1.3930 - val_accuracy: 0.7426 - val_loss: 0.8798\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.7361 - loss: 0.9034 - val_accuracy: 0.7523 - val_loss: 0.8497\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.7413 - loss: 0.8765 - val_accuracy: 0.7572 - val_loss: 0.8311\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.7448 - loss: 0.8618 - val_accuracy: 0.7588 - val_loss: 0.8153\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.7476 - loss: 0.8472 - val_accuracy: 0.7629 - val_loss: 0.7999\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.7528 - loss: 0.8329 - val_accuracy: 0.7690 - val_loss: 0.7848\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.7580 - loss: 0.8145 - val_accuracy: 0.7751 - val_loss: 0.7537\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.7647 - loss: 0.7848 - val_accuracy: 0.7821 - val_loss: 0.7285\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▅▆▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▃▃▃▂▂▂▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▇█</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.76626</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.72848</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.76626</td></tr><tr><td>final_val_accuracy</td><td>0.78208</td></tr><tr><td>loss</td><td>0.77915</td></tr><tr><td>total_flops</td><td>374784</td></tr><tr><td>total_params</td><td>18381</td></tr><tr><td>training_time</td><td>180.86946</td></tr><tr><td>val_accuracy</td><td>0.78208</td></tr><tr><td>val_loss</td><td>0.72848</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">robust-sweep-2</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/0r31b8ft' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/0r31b8ft</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_082035-0r31b8ft/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hwigli45 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_082353-hwigli45</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/hwigli45' target=\"_blank\">honest-sweep-3</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/hwigli45' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/hwigli45</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 16, Hidden dim: 256\nEncoder layers: 1, Decoder layers: 1\nCell type: LSTM, Dropout: 0.2\nTotal parameters: 568,045\nTotal FLOPs: 11,141,120\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 112ms/step - accuracy: 0.6936 - loss: 1.1958 - val_accuracy: 0.7601 - val_loss: 0.8091\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 112ms/step - accuracy: 0.7529 - loss: 0.8280 - val_accuracy: 0.7808 - val_loss: 0.7201\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 114ms/step - accuracy: 0.7740 - loss: 0.7476 - val_accuracy: 0.8014 - val_loss: 0.6486\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 115ms/step - accuracy: 0.7905 - loss: 0.6829 - val_accuracy: 0.8204 - val_loss: 0.5936\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 137ms/step - accuracy: 0.8057 - loss: 0.6309 - val_accuracy: 0.8310 - val_loss: 0.5477\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 119ms/step - accuracy: 0.8207 - loss: 0.5749 - val_accuracy: 0.8410 - val_loss: 0.5050\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 117ms/step - accuracy: 0.8364 - loss: 0.5211 - val_accuracy: 0.8607 - val_loss: 0.4440\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 118ms/step - accuracy: 0.8518 - loss: 0.4686 - val_accuracy: 0.8792 - val_loss: 0.3891\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▅▄▃▂▂▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▅▅▆▇█</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85587</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.38906</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.85587</td></tr><tr><td>final_val_accuracy</td><td>0.87918</td></tr><tr><td>loss</td><td>0.4566</td></tr><tr><td>total_flops</td><td>11141120</td></tr><tr><td>total_params</td><td>568045</td></tr><tr><td>training_time</td><td>658.08906</td></tr><tr><td>val_accuracy</td><td>0.87918</td></tr><tr><td>val_loss</td><td>0.38906</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">honest-sweep-3</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/hwigli45' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/hwigli45</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_082353-hwigli45/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sydm6xg4 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_083507-sydm6xg4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/sydm6xg4' target=\"_blank\">swept-sweep-4</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/sydm6xg4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/sydm6xg4</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 64, Hidden dim: 256\nEncoder layers: 2, Decoder layers: 2\nCell type: LSTM, Dropout: 0.3\nTotal parameters: 1,721,437\nTotal FLOPs: 26,214,400\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 284ms/step - accuracy: 0.6965 - loss: 1.1433 - val_accuracy: 0.7635 - val_loss: 0.7950\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 275ms/step - accuracy: 0.7635 - loss: 0.7890 - val_accuracy: 0.8123 - val_loss: 0.6225\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 279ms/step - accuracy: 0.8136 - loss: 0.6096 - val_accuracy: 0.8693 - val_loss: 0.4282\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 276ms/step - accuracy: 0.8685 - loss: 0.4264 - val_accuracy: 0.9098 - val_loss: 0.2975\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 273ms/step - accuracy: 0.9052 - loss: 0.3063 - val_accuracy: 0.9304 - val_loss: 0.2281\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 272ms/step - accuracy: 0.9284 - loss: 0.2319 - val_accuracy: 0.9400 - val_loss: 0.1944\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 276ms/step - accuracy: 0.9406 - loss: 0.1899 - val_accuracy: 0.9468 - val_loss: 0.1733\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 281ms/step - accuracy: 0.9479 - loss: 0.1621 - val_accuracy: 0.9465 - val_loss: 0.1685\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▄▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.94798</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.16852</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.94798</td></tr><tr><td>final_val_accuracy</td><td>0.94652</td></tr><tr><td>loss</td><td>0.16199</td></tr><tr><td>total_flops</td><td>26214400</td></tr><tr><td>total_params</td><td>1721437</td></tr><tr><td>training_time</td><td>1541.36688</td></tr><tr><td>val_accuracy</td><td>0.94652</td></tr><tr><td>val_loss</td><td>0.16852</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">swept-sweep-4</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/sydm6xg4' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/sydm6xg4</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_083507-sydm6xg4/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1da0cgf0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_090104-1da0cgf0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/1da0cgf0' target=\"_blank\">hearty-sweep-5</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/1da0cgf0' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/1da0cgf0</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 128, Hidden dim: 256\nEncoder layers: 2, Decoder layers: 2\nCell type: LSTM, Dropout: 0.3\nTotal parameters: 1,858,461\nTotal FLOPs: 31,457,280\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 290ms/step - accuracy: 0.6999 - loss: 1.1278 - val_accuracy: 0.7640 - val_loss: 0.7907\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 282ms/step - accuracy: 0.7704 - loss: 0.7635 - val_accuracy: 0.8331 - val_loss: 0.5421\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 283ms/step - accuracy: 0.8366 - loss: 0.5290 - val_accuracy: 0.8907 - val_loss: 0.3595\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 283ms/step - accuracy: 0.8925 - loss: 0.3483 - val_accuracy: 0.9280 - val_loss: 0.2395\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 289ms/step - accuracy: 0.9258 - loss: 0.2404 - val_accuracy: 0.9406 - val_loss: 0.1942\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 288ms/step - accuracy: 0.9410 - loss: 0.1892 - val_accuracy: 0.9475 - val_loss: 0.1709\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 286ms/step - accuracy: 0.9492 - loss: 0.1588 - val_accuracy: 0.9488 - val_loss: 0.1626\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 287ms/step - accuracy: 0.9544 - loss: 0.1399 - val_accuracy: 0.9517 - val_loss: 0.1528\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▅▆▇███</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▄▃▂▁▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.95375</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.15277</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.95375</td></tr><tr><td>final_val_accuracy</td><td>0.9517</td></tr><tr><td>loss</td><td>0.14132</td></tr><tr><td>total_flops</td><td>31457280</td></tr><tr><td>total_params</td><td>1858461</td></tr><tr><td>training_time</td><td>1597.12138</td></tr><tr><td>val_accuracy</td><td>0.9517</td></tr><tr><td>val_loss</td><td>0.15277</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hearty-sweep-5</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/1da0cgf0' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/1da0cgf0</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_090104-1da0cgf0/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vihxk2yq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_092756-vihxk2yq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/vihxk2yq' target=\"_blank\">misunderstood-sweep-6</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/vihxk2yq' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/vihxk2yq</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 128, Hidden dim: 256\nEncoder layers: 2, Decoder layers: 1\nCell type: GRU, Dropout: 0.1\nTotal parameters: 1,007,005\nTotal FLOPs: 23,199,744\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 179ms/step - accuracy: 0.7157 - loss: 1.0475 - val_accuracy: 0.7969 - val_loss: 0.6725\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 179ms/step - accuracy: 0.8153 - loss: 0.6139 - val_accuracy: 0.8872 - val_loss: 0.3759\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 178ms/step - accuracy: 0.8920 - loss: 0.3576 - val_accuracy: 0.9281 - val_loss: 0.2485\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 175ms/step - accuracy: 0.9271 - loss: 0.2398 - val_accuracy: 0.9403 - val_loss: 0.1994\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 176ms/step - accuracy: 0.9407 - loss: 0.1928 - val_accuracy: 0.9415 - val_loss: 0.1880\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 178ms/step - accuracy: 0.9468 - loss: 0.1675 - val_accuracy: 0.9477 - val_loss: 0.1733\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 179ms/step - accuracy: 0.9514 - loss: 0.1513 - val_accuracy: 0.9497 - val_loss: 0.1632\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.9546 - loss: 0.1380 - val_accuracy: 0.9499 - val_loss: 0.1622\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇████</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▅▃▂▁▁▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.95386</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.16218</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.95386</td></tr><tr><td>final_val_accuracy</td><td>0.94991</td></tr><tr><td>loss</td><td>0.14016</td></tr><tr><td>total_flops</td><td>23199744</td></tr><tr><td>total_params</td><td>1007005</td></tr><tr><td>training_time</td><td>996.6674</td></tr><tr><td>val_accuracy</td><td>0.94991</td></tr><tr><td>val_loss</td><td>0.16218</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">misunderstood-sweep-6</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/vihxk2yq' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/vihxk2yq</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_092756-vihxk2yq/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6n1ssi28 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_decoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_encoder_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_094450-6n1ssi28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/6n1ssi28' target=\"_blank\">cerulean-sweep-7</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/sweeps/mpq5soqm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/6n1ssi28' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/6n1ssi28</a>"},"metadata":{}},{"name":"stdout","text":"Embedding dim: 128, Hidden dim: 256\nEncoder layers: 2, Decoder layers: 1\nCell type: GRU, Dropout: 0.3\nTotal parameters: 1,007,005\nTotal FLOPs: 23,199,744\nEpoch 1/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 190ms/step - accuracy: 0.7152 - loss: 1.0636 - val_accuracy: 0.7932 - val_loss: 0.6926\nEpoch 2/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.8013 - loss: 0.6613 - val_accuracy: 0.8663 - val_loss: 0.4354\nEpoch 3/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 184ms/step - accuracy: 0.8725 - loss: 0.4221 - val_accuracy: 0.9152 - val_loss: 0.2891\nEpoch 4/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 181ms/step - accuracy: 0.9130 - loss: 0.2876 - val_accuracy: 0.9335 - val_loss: 0.2224\nEpoch 5/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 176ms/step - accuracy: 0.9320 - loss: 0.2232 - val_accuracy: 0.9418 - val_loss: 0.1931\nEpoch 6/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 175ms/step - accuracy: 0.9407 - loss: 0.1891 - val_accuracy: 0.9459 - val_loss: 0.1762\nEpoch 7/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 175ms/step - accuracy: 0.9465 - loss: 0.1690 - val_accuracy: 0.9490 - val_loss: 0.1664\nEpoch 8/8\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 175ms/step - accuracy: 0.9502 - loss: 0.1550 - val_accuracy: 0.9498 - val_loss: 0.1614\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>final_train_accuracy</td><td>▁</td></tr><tr><td>final_val_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▅▃▂▂▁▁▁</td></tr><tr><td>total_flops</td><td>▁</td></tr><tr><td>total_params</td><td>▁</td></tr><tr><td>training_time</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.94977</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.16139</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>final_train_accuracy</td><td>0.94977</td></tr><tr><td>final_val_accuracy</td><td>0.9498</td></tr><tr><td>loss</td><td>0.15592</td></tr><tr><td>total_flops</td><td>23199744</td></tr><tr><td>total_params</td><td>1007005</td></tr><tr><td>training_time</td><td>1005.06698</td></tr><tr><td>val_accuracy</td><td>0.9498</td></tr><tr><td>val_loss</td><td>0.16139</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cerulean-sweep-7</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/6n1ssi28' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/6n1ssi28</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_094450-6n1ssi28/logs</code>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Question 4","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import callbacks as keras_cb\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport wandb\nfrom wandb.integration.keras import WandbCallback\n\n# ─── A) Bring data & tokenizers back into scope ───────────────────\ntry:\n    pi = processed_data\n    input_tokenizer  = pi['input_tokenizer']\n    target_tokenizer = pi['target_tokenizer']\n    max_in, max_out = pi['max_in'], pi['max_out']\n    encoder_input_train = pi['encoder_input_train']\n    decoder_input_train = pi['decoder_input_train']\n    decoder_target_train= pi['decoder_target_train']\n    encoder_input_val   = pi['encoder_input_val']\n    decoder_input_val   = pi['decoder_input_val']\n    decoder_target_val  = pi['decoder_target_val']\n    print(\"✅ Data and tokenizers loaded from cache.\")\nexcept NameError:\n    train_data, val_data, test_data = load_dakshina_data(lang='hi')\n    pi = process_data(train_data, val_data)\n    input_tokenizer  = pi['input_tokenizer']\n    target_tokenizer = pi['target_tokenizer']\n    max_in, max_out = pi['max_in'], pi['max_out']\n    encoder_input_train = pi['encoder_input_train']\n    decoder_input_train = pi['decoder_input_train']\n    decoder_target_train= pi['decoder_target_train']\n    encoder_input_val   = pi['encoder_input_val']\n    decoder_input_val   = pi['decoder_input_val']\n    decoder_target_val  = pi['decoder_target_val']\n    print(\"✅ Data processed and tokenizers created.\")\nprint(\"✅ Step A complete\")\n\n# ─── B) Fetch best hyperparams ─────────────────────────────\napi    = wandb.Api()\nENTITY = \"mm21b044-indian-institute-of-technology-madras\"\nPROJ   = \"DA_seq2seq_transliteration\"\n\nprint(\"🔍 Fetching wandb runs...\")\nall_runs   = api.runs(f\"{ENTITY}/{PROJ}\")\nvalid_runs = [r for r in all_runs if 'final_val_accuracy' in r.summary]\nbest_run   = max(valid_runs, key=lambda r: r.summary['final_val_accuracy'])\nbest_cfg   = best_run.config\n\nprint(f\"✅ Best run: {best_run.id} with val_accuracy={best_run.summary['final_val_accuracy']:.4f}\")\nprint(\"Best sweep config:\", best_cfg)\n\ninp_vocab = len(input_tokenizer.word_index) + 1\ntgt_vocab = len(target_tokenizer.word_index) + 1\n\nBEST_HP = {\n    'input_vocab_size':   inp_vocab,\n    'target_vocab_size':  tgt_vocab,\n    'embedding_dim':      best_cfg['embedding_dim'],\n    'hidden_dim':         best_cfg['hidden_dim'],\n    'cell_type':          best_cfg['cell_type'],\n    'dropout_rate':       best_cfg['dropout_rate'],\n    'num_encoder_layers': best_cfg.get('num_encoder_layers', 1),\n    'num_decoder_layers': best_cfg.get('num_decoder_layers', 1)\n}\nprint(\"✅ Step B complete\")\n\n# ─── C) Rebuild model ─────────────────────────────────────────────\nprint(\"🔧 Rebuilding model...\")\nmodel = VanillaSeq2Seq(**BEST_HP)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\nprint(\"✅ Model built and compiled.\")\n\n# ─── D) Prepare for retraining ───────────────────────────────────\ny_train = np.expand_dims(decoder_target_train, -1)\ny_val   = np.expand_dims(decoder_target_val, -1)\n\nwandb.init(project=PROJ, name=\"q4_retrain_best_model\", reinit=True)\nwandb.config.update(BEST_HP, allow_val_change=True)\n\ncheckpoint_path = \"/kaggle/working/q4_best_model.weights.h5\"\ncb = [\n    keras_cb.ModelCheckpoint(\n        checkpoint_path,\n        save_best_only=True,\n        save_weights_only=True,\n        monitor='val_loss'\n    ),\n    WandbCallback(\n        save_model=False,\n        log_weights=True,\n        save_graph=False\n    )\n]\nprint(\"✅ Step D complete. Beginning training...\")\n\n# ─── E) Retrain ───────────────────────────────────────────────────\nhistory = model.model.fit(\n    [encoder_input_train, decoder_input_train],\n    y_train,\n    batch_size=wandb.config.get('batch_size', 64),\n    epochs=10,\n    validation_data=([encoder_input_val, decoder_input_val], y_val),\n    callbacks=cb,\n    verbose=1\n)\nprint(\"✅ Training complete\")\n\nwandb.finish()\nprint(\"✅ wandb finished\")\n\n# ─── F) Load weights and prepare test ─────────────────────────────\nmodel.model.load_weights(checkpoint_path)\nprint(\"✅ Weights loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:06:48.526122Z","iopub.execute_input":"2025-05-20T15:06:48.526657Z","iopub.status.idle":"2025-05-20T15:08:17.788260Z","shell.execute_reply.started":"2025-05-20T15:06:48.526635Z","shell.execute_reply":"2025-05-20T15:08:17.787702Z"}},"outputs":[{"name":"stdout","text":"✅ Data processed and tokenizers created.\n✅ Step A complete\n🔍 Fetching wandb runs...\n✅ Best run: s35s9ajw with val_accuracy=0.9523\nBest sweep config: {'cell_type': 'LSTM', 'hidden_dim': 256, 'dropout_rate': 0.1, 'embedding_dim': 128, 'num_decoder_layers': 1, 'num_encoder_layers': 2}\n✅ Step B complete\n🔧 Rebuilding model...\n✅ Model built and compiled.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_150650-q8n80qdx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/q8n80qdx' target=\"_blank\">q4_retrain_best_model</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/q8n80qdx' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/q8n80qdx</a>"},"metadata":{}},{"name":"stdout","text":"✅ Step D complete. Beginning training...\nEpoch 1/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 12ms/step - accuracy: 0.7129 - loss: 1.0550 - val_accuracy: 0.7994 - val_loss: 0.6679\nEpoch 2/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.8086 - loss: 0.6327 - val_accuracy: 0.8680 - val_loss: 0.4357\nEpoch 3/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.8765 - loss: 0.4036 - val_accuracy: 0.9099 - val_loss: 0.2952\nEpoch 4/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9195 - loss: 0.2655 - val_accuracy: 0.9378 - val_loss: 0.2096\nEpoch 5/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9388 - loss: 0.2000 - val_accuracy: 0.9453 - val_loss: 0.1796\nEpoch 6/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9477 - loss: 0.1666 - val_accuracy: 0.9488 - val_loss: 0.1638\nEpoch 7/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9526 - loss: 0.1477 - val_accuracy: 0.9519 - val_loss: 0.1548\nEpoch 8/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.9559 - loss: 0.1333 - val_accuracy: 0.9511 - val_loss: 0.1523\nEpoch 9/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9592 - loss: 0.1212 - val_accuracy: 0.9539 - val_loss: 0.1450\nEpoch 10/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9608 - loss: 0.1142 - val_accuracy: 0.9540 - val_loss: 0.1425\n✅ Training complete\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇██████</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.95989</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.14247</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.11669</td></tr><tr><td>val_accuracy</td><td>0.95404</td></tr><tr><td>val_loss</td><td>0.14247</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">q4_retrain_best_model</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/q8n80qdx' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/q8n80qdx</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_150650-q8n80qdx/logs</code>"},"metadata":{}},{"name":"stdout","text":"✅ wandb finished\n✅ Weights loaded\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"test_texts = test_data['latin'].astype(str).tolist()\ntrue_texts = test_data['native'].astype(str).tolist()\n\nenc_test = pad_sequences(\n    input_tokenizer.texts_to_sequences(test_texts),\n    maxlen=max_in, padding='post'\n)\nstart_idx = target_tokenizer.texts_to_sequences(['\\t'])[0][0]\nindex_to_char = {i: ch for ch, i in target_tokenizer.word_index.items()}\nindex_to_char[0] = ''\n\ndef decode(seq):\n    seq = seq[np.newaxis]\n    dec_input = np.array([[start_idx]])\n    out = ''\n    for _ in range(max_out):\n        preds = model.model.predict([seq, dec_input], verbose=0)\n        nxt = np.argmax(preds[0, -1, :])\n        if nxt==0 or index_to_char[nxt]=='\\n': break\n        out += index_to_char[nxt]\n        dec_input = np.concatenate([dec_input, [[nxt]]], axis=1)\n    return out\nprint(\"🧠 Starting decoding...\")\npreds, corr = [], 0\n\nfor i, s in enumerate(tqdm(enc_test, desc=\"Decoding examples\")):\n    p = decode(s)\n    preds.append(p)\n    if p == true_texts[i]:\n        corr += 1\n\nprint(\"✅ Decoding done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T14:14:16.986176Z","iopub.execute_input":"2025-05-20T14:14:16.986631Z","iopub.status.idle":"2025-05-20T15:00:21.996152Z","shell.execute_reply.started":"2025-05-20T14:14:16.986611Z","shell.execute_reply":"2025-05-20T15:00:21.995517Z"}},"outputs":[{"name":"stdout","text":"🧠 Starting decoding...\n","output_type":"stream"},{"name":"stderr","text":"Decoding examples: 100%|██████████| 4502/4502 [45:57<00:00,  1.63it/s] \n","output_type":"stream"},{"name":"stdout","text":"✅ Decoding done\n✅ Final test exact-match accuracy: 3.5318%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_150014-b1ic3mri</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/b1ic3mri' target=\"_blank\">q4_test_eval</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/b1ic3mri' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/b1ic3mri</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.03532</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">q4_test_eval</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/b1ic3mri' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/b1ic3mri</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_150014-b1ic3mri/logs</code>"},"metadata":{}},{"name":"stdout","text":"✅ Test accuracy logged to wandb\n✅ Saved predictions to /kaggle/working/predictions_vanilla\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3273880890.py:38: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.tight_layout()\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Matplotlib currently does not support Devanagari natively.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  fig.canvas.print_figure(bytes_io, **kw)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 900x900 with 9 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"✅ Plot displayed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"test_acc = corr / len(enc_test)\nprint(f\"✅ Final test exact-match accuracy: {test_acc:.4%}\")\n\n# ─── G) Log test result ───────────────────────────────────────────\nwandb.init(project=PROJ, name=\"q4_test_eval\", reinit=True)\nwandb.log({\"test_accuracy\": test_acc})\nwandb.finish()\nprint(\"✅ Test accuracy logged to wandb\")\n\n# ─── H) Save & display predictions ───────────────────────────────\nout_dir = \"/kaggle/working/predictions_vanilla\"\nos.makedirs(out_dir, exist_ok=True)\nimport pandas as pd\nfrom IPython.display import HTML, display\n\ndf = pd.DataFrame({\n    \"Latin\":      test_texts[:9],\n    \"Reference\":  true_texts[:9],\n    \"Prediction\": preds[:9]\n})\n\n# Show as HTML table\ndisplay(HTML(df.to_html(index=False)))\n\nfig, axes = plt.subplots(3,3,figsize=(9,9))\nfor ax, idx in zip(axes.flatten(), range(9)):\n    txt = f\"In:  {test_texts[idx]}\\nRef: {true_texts[idx]}\\nPred:{preds[idx]}\"\n    ax.text(0,0.1, txt, wrap=True)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\nprint(\"✅ Plot displayed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T15:10:56.195504Z","iopub.execute_input":"2025-05-20T15:10:56.196172Z","iopub.status.idle":"2025-05-20T15:11:03.498260Z","shell.execute_reply.started":"2025-05-20T15:10:56.196148Z","shell.execute_reply":"2025-05-20T15:11:03.497278Z"}},"outputs":[{"name":"stdout","text":"✅ Final test exact-match accuracy: 3.5318%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_151056-33ptrpg2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/33ptrpg2' target=\"_blank\">q4_test_eval</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/33ptrpg2' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/33ptrpg2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.03532</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">q4_test_eval</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/33ptrpg2' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/33ptrpg2</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_151056-33ptrpg2/logs</code>"},"metadata":{}},{"name":"stdout","text":"✅ Test accuracy logged to wandb\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Latin</th>\n      <th>Reference</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>अंक</td>\n      <td>ank</td>\n      <td>amat</td>\n    </tr>\n    <tr>\n      <td>अंक</td>\n      <td>anka</td>\n      <td>amat</td>\n    </tr>\n    <tr>\n      <td>अंकित</td>\n      <td>ankit</td>\n      <td>antati</td>\n    </tr>\n    <tr>\n      <td>अंकों</td>\n      <td>anakon</td>\n      <td>anaron</td>\n    </tr>\n    <tr>\n      <td>अंकों</td>\n      <td>ankhon</td>\n      <td>anaron</td>\n    </tr>\n    <tr>\n      <td>अंकों</td>\n      <td>ankon</td>\n      <td>anaron</td>\n    </tr>\n    <tr>\n      <td>अंकोर</td>\n      <td>angkor</td>\n      <td>anrur</td>\n    </tr>\n    <tr>\n      <td>अंकोर</td>\n      <td>ankor</td>\n      <td>anrur</td>\n    </tr>\n    <tr>\n      <td>अंगारक</td>\n      <td>angaarak</td>\n      <td>angraar</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2309 (\\N{DEVANAGARI LETTER A}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Matplotlib currently does not support Devanagari natively.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2306 (\\N{DEVANAGARI SIGN ANUSVARA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2325 (\\N{DEVANAGARI LETTER KA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2367 (\\N{DEVANAGARI VOWEL SIGN I}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2340 (\\N{DEVANAGARI LETTER TA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2379 (\\N{DEVANAGARI VOWEL SIGN O}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2352 (\\N{DEVANAGARI LETTER RA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2327 (\\N{DEVANAGARI LETTER GA}) missing from current font.\n  plt.tight_layout()\n/tmp/ipykernel_35/3018021838.py:30: UserWarning: Glyph 2366 (\\N{DEVANAGARI VOWEL SIGN AA}) missing from current font.\n  plt.tight_layout()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 900x900 with 9 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"✅ Plot displayed\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Question 5","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 6","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model\n\n# ─── A) Build sub‑models to extract encoder & decoder hidden states ─────────\n# Assume `model` is your trained AttentionSeq2Seq or VanillaSeq2Seq.model\n# and that its layers are named 'enc_lstm_0', 'dec_lstm_step' (or equivalent).\n\n# 1) Encoder model: maps encoder inputs → all hidden states\nenc_input  = model.input[0]  # encoder_input\nenc_layer  = model.get_layer('enc_lstm_0')  # first encoder LSTM\nenc_outputs, enc_h, enc_c = enc_layer.output, *enc_layer.states\nencoder_model = Model(enc_input, enc_outputs)  \n\n# 2) Decoder model: at each step, map [prev token, prev state] → [output, new state]\n#    We reuse the same LSTM cell for each time‑step.\ndec_input_token = model.input[1]  # decoder_input (full sequence)\n# We will feed one time‑step at a time, so create placeholders:\ndec_state_h = tf.keras.Input(shape=(H,), name='dec_state_h')\ndec_state_c = tf.keras.Input(shape=(H,), name='dec_state_c')\ndec_embed   = model.get_layer('embedding_1')(dec_input_token)  # your decoder Embedding\ndec_lstm    = model.get_layer('dec_lstm_step')  \n# slice out only the last time‑step from dec_embed:\nlast_token = dec_embed[:, -1:, :]  # shape (batch, 1, emb)\ndec_out, new_h, new_c = dec_lstm(last_token, initial_state=[dec_state_h, dec_state_c])\ndecoder_model = Model(\n    [dec_input_token, dec_state_h, dec_state_c],\n    [dec_out, new_h, new_c]\n)\n\n# ─── B) Run through test set, collect states ────────────────────────────────\n# Prepare test inputs as before\nenc_test = encoder_input_test  # shape (N, T_enc)\ndec_start = np.full((len(enc_test), 1), start_token_idx)\n\n# 1) Get encoder hidden states for all examples\nenc_all_states = encoder_model.predict(enc_test, batch_size=64)  \n# shape (N, T_enc, H)\n\n# 2) For each decoder step i, collect decoder hidden state across examples\nmax_steps = max_out - 1\ndec_states = np.zeros((len(enc_test), max_steps, H))\n\n# Initial decoder states = zero or encoder final states?\n# Here we initialize from zeros; adjust if you used encoder states to init decoder.\nstate_h = np.zeros((len(enc_test), H))\nstate_c = np.zeros((len(enc_test), H))\n\n# Step through decoder\nfor t in range(max_steps):\n    # feed dec_start for first step, then zeros (we just want states)\n    inp = dec_start if t == 0 else np.zeros_like(dec_start)\n    out, state_h, state_c = decoder_model.predict([inp, state_h, state_c],\n                                                 batch_size=64)\n    dec_states[:, t, :] = state_h\n    # next inp token ignored because we just care about states\n\n# ─── C) Compute connectivity (correlation) matrices ────────────────────────\n# For each decoder step t and each encoder time-step s, compute Pearson corr across batch\nconn = np.zeros((max_steps, max_in))  # connectivity map\n\nfor t in range(max_steps):\n    for s in range(max_in):\n        # correlation across N examples between enc_all_states[:,s,:] and dec_states[:,t,:]\n        # we compute mean over hidden dims:\n        num = np.mean((enc_all_states[:, s, :] - enc_all_states[:, s, :].mean(axis=0)) *\n                      (dec_states[:, t, :] - dec_states[:, t, :].mean(axis=0)), axis=1)\n        den = np.std(enc_all_states[:, s, :], axis=1) * np.std(dec_states[:, t, :], axis=1) + 1e-8\n        corr = np.mean(num/den)\n        conn[t, s] = corr\n\n# ─── D) Plot first 9 decoder‑step connectivity heatmaps ────────────────────\nfig, axes = plt.subplots(3, 3, figsize=(12, 12))\nfor idx, ax in enumerate(axes.flatten()):\n    if idx >= max_steps: break\n    im = ax.imshow(conn[idx], aspect='auto', cmap='viridis')\n    ax.set_title(f\"Decoder Step {idx+1}\")\n    ax.set_xlabel(\"Encoder time‑step\")\n    ax.set_ylabel(\"Correlation\")\nfig.colorbar(im, ax=axes.ravel().tolist())\nplt.suptitle(\"Connectivity: which input positions relate to each output step\")  \nplt.tight_layout(rect=[0,0,1,0.96])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}