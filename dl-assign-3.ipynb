{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11873693,"sourceType":"datasetVersion","datasetId":7462032}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, GRU, RNN, Dense, Dropout","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:28:28.998094Z","iopub.execute_input":"2025-05-19T16:28:28.998401Z","iopub.status.idle":"2025-05-19T16:28:53.790718Z","shell.execute_reply.started":"2025-05-19T16:28:28.998372Z","shell.execute_reply":"2025-05-19T16:28:53.789374Z"}},"outputs":[{"name":"stderr","text":"2025-05-19 16:28:33.012055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747672113.279346      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747672113.366128      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:45:45.064557Z","iopub.execute_input":"2025-05-19T16:45:45.064861Z","iopub.status.idle":"2025-05-19T16:46:11.026283Z","shell.execute_reply.started":"2025-05-19T16:45:45.064840Z","shell.execute_reply":"2025-05-19T16:46:11.025122Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\nCollecting wandb\n  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nDownloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.9\n    Uninstalling wandb-0.19.9:\n      Successfully uninstalled wandb-0.19.9\nSuccessfully installed wandb-0.19.11\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import wandb\nfrom wandb.integration.keras import WandbCallback\n\nwandb.login(key='e030007b097df00d9a751748294abc8440f932b1')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:48:49.378940Z","iopub.execute_input":"2025-05-19T16:48:49.379335Z","iopub.status.idle":"2025-05-19T16:48:56.126217Z","shell.execute_reply.started":"2025-05-19T16:48:49.379306Z","shell.execute_reply":"2025-05-19T16:48:56.125446Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vanilla_lstm_run_q1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/notojlv9' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/notojlv9</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_164338-notojlv9/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_164849-lyrt0b9h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/lyrt0b9h' target=\"_blank\">vanilla_lstm_run_q1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/lyrt0b9h' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/lyrt0b9h</a>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/lyrt0b9h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d5b85474c10>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def load_data(file_path):\n    df = pd.read_csv(file_path, sep='\\t', header=None, names=['latin', 'native'])\n    df = df.dropna()\n    df['latin'] = df['latin'].astype(str)\n    df['native'] = df['native'].astype(str)\n    return df\n\ndef load_dakshina_dataset(language_code='hi', base_dir='/kaggle/input/dak-data/dakshina_dataset_v1.0'):\n    path = os.path.join(base_dir, language_code, 'lexicons')\n    return (\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.train.tsv')),\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.dev.tsv')),\n        load_data(os.path.join(path, f'{language_code}.translit.sampled.test.tsv')),\n    )\n\ntrain_data, val_data, test_data = load_dakshina_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:34:25.604196Z","iopub.execute_input":"2025-05-19T16:34:25.604571Z","iopub.status.idle":"2025-05-19T16:34:25.717761Z","shell.execute_reply.started":"2025-05-19T16:34:25.604543Z","shell.execute_reply":"2025-05-19T16:34:25.716821Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(train_data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:34:26.488732Z","iopub.execute_input":"2025-05-19T16:34:26.489132Z","iopub.status.idle":"2025-05-19T16:34:26.493672Z","shell.execute_reply.started":"2025-05-19T16:34:26.489104Z","shell.execute_reply":"2025-05-19T16:34:26.492783Z"}},"outputs":[{"name":"stdout","text":"(44202, 2)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"wandb.init(\n    project=\"DA_seq2seq_transliteration\",\n    name=\"vanilla_lstm_run_q1\",\n    config={\n        \"model_type\": \"vanilla\",\n        \"cell_type\": \"LSTM\",\n        \"embedding_dim\": 64,\n        \"hidden_dim\": 128,\n        \"dropout_rate\": 0.2,\n        \"batch_size\": 64,\n        \"epochs\": 10,\n        \"input_vocab_size\": len(input_tokenizer.word_index) + 1,\n        \"target_vocab_size\": len(target_tokenizer.word_index) + 1,\n        \"max_input_length\": max_in,\n        \"max_target_length\": max_out,\n        \"optimizer\": \"adam\",\n        \"loss\": \"sparse_categorical_crossentropy\",\n        \"dataset\": \"dakshina_hi\"\n    }\n)\n\nclass VanillaSeq2Seq:\n    def __init__(self, input_vocab_size, target_vocab_size, embedding_dim, hidden_dim, cell_type='LSTM', dropout_rate=0.2):\n        self.input_vocab_size = input_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.cell_type = cell_type\n        self.dropout_rate = dropout_rate\n        self.model = self._build_model()\n\n    def _get_rnn_cell(self, return_sequences, return_state):\n        if self.cell_type == 'LSTM':\n            return LSTM(self.hidden_dim, return_sequences=return_sequences, return_state=return_state)\n        elif self.cell_type == 'GRU':\n            return GRU(self.hidden_dim, return_sequences=return_sequences, return_state=return_state)\n        else:\n            return RNN(self.hidden_dim, return_sequences=return_sequences, return_state=return_state)\n\n    def _build_model(self):\n        encoder_inputs = Input(shape=(None,), name='encoder_input')\n        decoder_inputs = Input(shape=(None,), name='decoder_input')\n\n        encoder_embed = Embedding(self.input_vocab_size, self.embedding_dim)(encoder_inputs)\n        encoder_embed = Dropout(self.dropout_rate)(encoder_embed)\n\n        if self.cell_type == 'LSTM':\n            encoder_outputs, state_h, state_c = LSTM(self.hidden_dim, return_state=True)(encoder_embed)\n            encoder_states = [state_h, state_c]\n        else:\n            encoder_outputs, state_h = self._get_rnn_cell(return_sequences=False, return_state=True)(encoder_embed)\n            encoder_states = [state_h]\n\n        decoder_embed = Embedding(self.target_vocab_size, self.hidden_dim)(decoder_inputs)\n        decoder_embed = Dropout(self.dropout_rate)(decoder_embed)\n\n        if self.cell_type == 'LSTM':\n            decoder_outputs, _, _ = LSTM(self.hidden_dim, return_sequences=True, return_state=True)(decoder_embed, initial_state=encoder_states)\n        else:\n            decoder_outputs, _ = self._get_rnn_cell(return_sequences=True, return_state=True)(decoder_embed, initial_state=encoder_states)\n\n        decoder_dense = Dense(self.target_vocab_size, activation='softmax')\n        decoder_outputs = decoder_dense(decoder_outputs)\n\n        return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    def compile(self):\n        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    def fit(self, train_data, val_data, batch_size=64, epochs=10, callbacks=None):\n        return self.model.fit(\n            [train_data['encoder_input'], train_data['decoder_input']],\n            np.expand_dims(train_data['decoder_target'], -1),\n            validation_data=(\n                [val_data['encoder_input'], val_data['decoder_input']],\n                np.expand_dims(val_data['decoder_target'], -1)\n            ),\n            batch_size=batch_size,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n\n# ✅ 7. Initialize and train model\ninput_vocab_size = len(input_tokenizer.word_index) + 1\ntarget_vocab_size = len(target_tokenizer.word_index) + 1\n\nmodel = VanillaSeq2Seq(\n    input_vocab_size=input_vocab_size,\n    target_vocab_size=target_vocab_size,\n    embedding_dim=64,\n    hidden_dim=128,\n    cell_type='LSTM',\n    dropout_rate=0.2\n)\n\nmodel.compile()\n\nwandb_callback = WandbCallback(\n    log_model=False,           # no wandb artifact\n    save_graph=False,          # don't try to render graph\n    save_model=False           # ✅ disables all auto saving\n)\n\nmodel.fit(\n    train_data={\n        'encoder_input': encoder_input_train,\n        'decoder_input': decoder_input_train,\n        'decoder_target': decoder_target_train\n    },\n    val_data={\n        'encoder_input': encoder_input_val,\n        'decoder_input': decoder_input_val,\n        'decoder_target': decoder_target_val\n    },\n    batch_size=64,\n    epochs=10,\n    callbacks=[wandb_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T17:09:31.389544Z","iopub.execute_input":"2025-05-19T17:09:31.389874Z","iopub.status.idle":"2025-05-19T17:13:34.844141Z","shell.execute_reply.started":"2025-05-19T17:09:31.389850Z","shell.execute_reply":"2025-05-19T17:13:34.843402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.81783</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>0.3863</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.44345</td></tr><tr><td>val_accuracy</td><td>0.82446</td></tr><tr><td>val_loss</td><td>0.3863</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vanilla_lstm_run_q1</strong> at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/5tbvofqf' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/5tbvofqf</a><br> View project at: <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_170626-5tbvofqf/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_170931-y7e9ir4w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/y7e9ir4w' target=\"_blank\">vanilla_lstm_run_q1</a></strong> to <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/y7e9ir4w' target=\"_blank\">https://wandb.ai/mm21b044-indian-institute-of-technology-madras/DA_seq2seq_transliteration/runs/y7e9ir4w</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 34ms/step - accuracy: 0.7929 - loss: 0.5777 - val_accuracy: 0.8266 - val_loss: 0.3852\nEpoch 2/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8264 - loss: 0.3971 - val_accuracy: 0.8274 - val_loss: 0.3826\nEpoch 3/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8291 - loss: 0.3908 - val_accuracy: 0.8340 - val_loss: 0.3777\nEpoch 4/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8315 - loss: 0.3884 - val_accuracy: 0.8352 - val_loss: 0.3720\nEpoch 5/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.8348 - loss: 0.3868 - val_accuracy: 0.8373 - val_loss: 0.3726\nEpoch 6/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8374 - loss: 0.3820 - val_accuracy: 0.8389 - val_loss: 0.3699\nEpoch 7/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8370 - loss: 0.3807 - val_accuracy: 0.8413 - val_loss: 0.3653\nEpoch 8/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8407 - loss: 0.3751 - val_accuracy: 0.8430 - val_loss: 0.3645\nEpoch 9/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 33ms/step - accuracy: 0.8419 - loss: 0.3718 - val_accuracy: 0.8454 - val_loss: 0.3618\nEpoch 10/10\n\u001b[1m691/691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 34ms/step - accuracy: 0.8450 - loss: 0.3692 - val_accuracy: 0.8460 - val_loss: 0.3609\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d5b312bbdd0>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}